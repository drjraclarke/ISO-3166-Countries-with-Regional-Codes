{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "12m_RjeFlgW4OKJdkCMHT81pxKCb0vKmX",
      "authorship_tag": "ABX9TyNXUV3wUycoVfMNAGr3rFp4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/drjraclarke/ISO-3166-Countries-with-Regional-Codes/blob/master/splicing_tool_1_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **In silico deep mutational scanning with Splice AI v. 1.2 **\n",
        "**This end-to-end tool visualises the predicted regulatory and modulatory architecture of alternative exon splicing for exons and neighbouring intronic regions queried by you.** It performs in silico deep mutational scanning using [SPLICE AI (Jaganathan et al)](https://doi.org/10.1016/j.cell.2018.12.015), a deep convolutional neural network (CNN) that predicts splice junctions from primary pre-mRNA transcripts with high accuracy. Splice AI DIM handles the genomic sequence queries, indel library preparation, running of the CNN and performs downstream analyses for you using a Google colabotory hosted GPU. Google colaboratory free subscriptions do have limited memory and runtime which can vary with community usage.\n",
        "\n",
        "The output is visualised as a series of heatmaps of all deletions and substituions, and a curve fitted to 1-6nt length deletions. There are options to: change the nucelotide windown splice AI scans either side of each variant, vary the size of the indel library (and resultant plots) and vary the number of nucelotides in 5' and 3' intronic regions that are included in the analysis.\n",
        "\n",
        "**How to use**\n",
        "1.   Log into google account\n",
        "2.   Select \"Runtime\" > \"Change run time type\" and select any available option with \"GPU\" or \"TPU\".\n",
        "3.   Build query from options in below panel. Default options produce optimal plotting. Advised to select download option so model output is saved once complete in case connection becomes idle. Ensembl exon ID not needed if gene and exon number known.\n",
        "3.   Run all cells (\"Runtime\" > \"Run all\")\n",
        "5.   Visualise output in notebook and/or locally on your machine (if download option selected for)\n",
        "\n",
        "**FAQs/troubleshooting**\n",
        "\n",
        "*   Make sure your gene name and exon number correspond to the Ensembl database naming. If in doubt or errors arise, search the Ensembl database for your gene, navigate to the primary transcript, find the list of exons and see that your query aligns with their exon ranking. This tool queries Ensembl for you but does require precise name and exon matching with no syntax errors. Note Ensembl does not take exon numerical_alphabateical names as input (eg. Exon 2B).\n",
        "*   The compute time will increase substantially (from approx 5-10 mins to 60-100 mins) with increasing size of the indel library used as input to SpliceAI. Indel library size is determined by the customisable options and expansion of 3 & 5 prime ends of the exon. Caution when substantially expanding the intronic regions, this may mean the neighbouring exon is included. This can be checked at Ensembl by knowing the size of the introns in question.\n",
        "*   The output files will be named according to your specific query. They will be in your downloads folder. There may be a google notification to allow mutliple downloads which requires your agreement. They will include a query csv file detailing the options you selected.\n",
        "*   Any feedback, queries or problems please do contact Joe Clarke at jc58@sanger.ac.uk. Ideally use the subject header \"SpliceAI tool query\".  \n",
        "*   If the code and output from each cell are visible and you wish to hide them, you can do so by double clikcing on the title of the cell. Alternatively, \"Edit\" > \"Select all cells\" > \"View\" > \"Show/hide code\" > \"Show/hide output\".\n",
        "*   No recall thresholds are assigned in this tool for the Splice AI output. For a detailed discussion regarding thresholds and guidance on the precision/recall trade-offs, read the publication by Jaganathan et al.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*With thanks to Pablo Baeza-Centurion,Belen Minana, Andre J. Faure, Mike Thompson, Sophie Bonnal,Gioia Quarantini, Juan Valcarcel and Ben Lehner at the Centre for Genomic Regulation (Barcelona, Spain) and Wellcome Sanger Institute (Cambridge, UK) for developing the methods behind this tool. Thanks to the team at SPLICE AI and [SPLICE AI VISUAL](https://doi.org/10.1186/s40246-023-00451-1)/BATCH SPLICE for their research and existing colaboratory notebook which was used as a guide for this tool development.*\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "O5qBATZapPm5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "GRCh = \"38\"  #@param [37, 38]\n",
        "if GRCh == \"37\":\n",
        "  hg = \"19\"\n",
        "if GRCh == \"38\":\n",
        "  hg = GRCh\n",
        "\n",
        "window =  125#@param {type:\"integer\"}\n",
        "#@markdown  * Must be between 50-4999 (max distance between the variant and gained/lost splice site predicted by SpliceAI (default: 50). <br/>\n",
        "automatic_download = True #@param {type:\"boolean\"}\n",
        "#@markdown  * `GRCh` : 37 or 38 <br/>\n",
        "#@markdown * `window` : must be between `50` et `4999`\n",
        "#@markdown * `automatic_download` : <img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" width=20> automatically\n",
        "#@markdown download the plots and metadata when finished\n",
        "if window > 4999:\n",
        "  print(\"ERROR : window for splice AI analysis must be between 50 and 4999\")\n",
        "\n",
        "  # Text input for \"Ensembl Gene Name\"\n",
        "ensembl_gene_name_query = \"ST7\" #@param {type:\"string\"}\n",
        "\n",
        "# Input for \"Exon Number\"\n",
        "exon_number_query = 7 #@param {type:\"integer\"}\n",
        "\n",
        "# Optional input for ensembl exon ID if already known\n",
        "ensembl_exon_id = \"\"#@param {type:\"string\"}\n",
        "#@markdown  * not necessary if gene and exon number known <br/>\n",
        "# Indel library options\n",
        "default_deletion_sizes  = True #@param {type:\"boolean\"}\n",
        "#@markdown  * Default deletion lengths = (1-6nt, 18nt, 21nt) <br/>\n",
        "custom_deletion_sizes  = \"1,2,3,4,5,6,7,8,9,10,21\" #@param {type:\"string\"}\n",
        "#@markdown  * Separate by a comma eg. \"1,3,5\". Note, 1-6nt deletions (1,2,3,4,5,6) minimum required for plotting. <br/>\n",
        "include_all_possible_substitutions = True #@param {type: \"boolean\"}\n",
        "\n",
        "#Input for \"Intronic_region_3prime\"\n",
        "expand_3prime = 25 #@param {type:\"integer\"}\n",
        "\n",
        "#Input fpr \"Intronic_region_5prime\"\n",
        "expand_5prime = 70 #@param {type:\"integer\"}\n",
        "# Display the values\n",
        "print(f\"Ensembl Gene Name: {ensembl_gene_name_query}\")\n",
        "print(f\"Exon Number: {exon_number_query}\")\n",
        "print(f\"Ensembl exon ID : {ensembl_exon_id}\")\n",
        "print(f\"Intronic 3 prime inclusion : {expand_3prime} nucleotides\")\n",
        "print(f\"Intronic 5 prime inclusion : {expand_5prime} nucelotides\")\n",
        "print(\"Indel library:\")\n",
        "if default_deletion_sizes :\n",
        "    print(\" - Deletions: 1-6nt, 18nt, 21nt\")\n",
        "else:\n",
        "    # Assuming custom_deletion_sizes is a string of numbers separated by commas\n",
        "    print(f\" - Deletions: {custom_deletion_sizes} nt\")\n",
        "\n",
        "if include_all_possible_substitutions:\n",
        "    print(\" - Substitutions: Yes\")\n",
        "else:\n",
        "    print(\" - Substitutions: No\")\n",
        "\n",
        "deletion_sizes = \"1,2,3,4,5,6,18,21\" if default_deletion_sizes else custom_deletion_sizes\n",
        "\n",
        "# Define the headers and the row of data\n",
        "headers = [\"Ensembl Gene Name\", \"Exon Number\",\"Ensembl Exon ID\", \"Intronic 3 prime inclusion\", \"Intronic 5 prime inclusion\", \"Deletions\", \"Substitutions\"]\n",
        "data_row = [\n",
        "    ensembl_gene_name_query,\n",
        "    exon_number_query,\n",
        "    f\"{expand_3prime} nt\",\n",
        "    f\"{expand_5prime} nt\",\n",
        "    deletion_sizes,\n",
        "    \"Yes\" if include_all_possible_substitutions else \"No\"\n",
        "]\n",
        "\n",
        "# Write to a CSV file\n",
        "query_csv_file_path = f\"{ensembl_gene_name_query}_{exon_number_query}_query.csv\"\n",
        "with open(query_csv_file_path, mode='w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(headers)\n",
        "    writer.writerow(data_row)"
      ],
      "metadata": {
        "id": "4d0CcccxT_5K",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install required libraries, modules and reference files. Load functions.\n",
        "# Install required libraries\n",
        "%pip install biopython\n",
        "%pip install pyfastx\n",
        "%pip install tensorflow\n",
        "%pip install spliceai\n",
        "%pip install ensembl_rest\n",
        "\n",
        "# Import the necessary modules\n",
        "import ensembl_rest\n",
        "import pandas as pd\n",
        "import requests\n",
        "import sys\n",
        "from Bio.Seq import Seq\n",
        "import pyfastx\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "\n",
        "!wget -nc http://hgdownload.soe.ucsc.edu/goldenPath/hg{hg}/bigZips/hg{hg}.fa.gz\n",
        "!gzip -df hg{hg}.fa.gz\n",
        "hgfastafile = pyfastx.Fasta('hg{0}.fa'.format(hg))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "c4F6yS9L-Fdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load functions\n",
        "def generate_all_singles(wt_sequence, exon_id, chromosome, padded_chrom_start):\n",
        "    wt_sequence_list = list(wt_sequence)\n",
        "    my_singles = []\n",
        "    my_ids = []\n",
        "    wt_sequences = []  # To store the WT sequence for each variant\n",
        "\n",
        "    for i, nucleotide in enumerate(wt_sequence_list):\n",
        "        possible_substitutions = [nt for nt in [\"A\", \"T\", \"G\", \"C\"] if nt != nucleotide]\n",
        "        for each_substitution in possible_substitutions:\n",
        "            new_single_list = wt_sequence_list.copy()\n",
        "            new_single_list[i] = each_substitution\n",
        "            new_single = \"\".join(new_single_list)\n",
        "            my_ids.append(f\"chr{chromosome}:{padded_chrom_start}_ex{exon_id}_Sub_{nucleotide}{i + 1}{each_substitution}\")\n",
        "            my_singles.append(new_single)\n",
        "            wt_sequences.append(wt_sequence)  # Add the WT sequence\n",
        "\n",
        "    return pd.DataFrame({\n",
        "        'ID': my_ids,\n",
        "        'Sequence': my_singles,\n",
        "        'Chromosome': [chromosome] * len(my_ids),\n",
        "        'WT_sequence': wt_sequences\n",
        "    })\n",
        "\n",
        "def generate_all_deletions(k, WT, exon_id, chromosome, padded_chrom_start):\n",
        "    WT_vectorized = list(WT)\n",
        "    number_of_deletions = len(WT_vectorized) - k + 1\n",
        "    sequences = []\n",
        "    IDs = []\n",
        "    wt_sequences = []  # To store the WT sequence for each variant\n",
        "\n",
        "    for i in range(number_of_deletions):\n",
        "        deleted_positions = range(i, i + k)\n",
        "        sequence_with_deletion = ''.join([WT_vectorized[j] for j in range(len(WT_vectorized)) if j not in deleted_positions])\n",
        "        ID = f\"chr{chromosome}:{padded_chrom_start}_ex{exon_id}_Del_k{k}_{i + 1}-{i + k}\"\n",
        "        sequences.append(sequence_with_deletion)\n",
        "        IDs.append(ID)\n",
        "        wt_sequences.append(WT)  # Add the WT sequence\n",
        "\n",
        "    return pd.DataFrame({\n",
        "        'ID': IDs,\n",
        "        'Sequence': sequences,\n",
        "        'Chromosome': [chromosome] * len(IDs),\n",
        "        'WT_sequence': wt_sequences\n",
        "    })\n",
        "\n",
        "def remove_duplicates_in_variant_data_frame(variant_data_frame):\n",
        "    unique_sequences = {}\n",
        "\n",
        "    # Adjusting the loop to also iterate over the WT_sequence column\n",
        "    for ID, sequence, chromosome, WT_sequence in zip(variant_data_frame['ID'], variant_data_frame['Sequence'], variant_data_frame['Chromosome'], variant_data_frame['WT_sequence']):\n",
        "        unique_key = (sequence, chromosome, WT_sequence)  # Adding WT_sequence to the unique key\n",
        "        if unique_key not in unique_sequences:\n",
        "            unique_sequences[unique_key] = {'IDs': [ID], 'Chromosome': chromosome, 'WT_sequence': WT_sequence}\n",
        "        else:\n",
        "            unique_sequences[unique_key]['IDs'].append(ID)\n",
        "\n",
        "    # Adjusting the structure of the new_data_frame to include WT_sequence\n",
        "    new_data_frame = {'Sequence': [], 'ID': [], 'Chromosome': [], 'WT_sequence': []}\n",
        "\n",
        "    for (sequence, _, WT_sequence), details in unique_sequences.items():\n",
        "        new_data_frame['Sequence'].append(sequence)\n",
        "        new_data_frame['ID'].append(';'.join(details['IDs']))\n",
        "        new_data_frame['Chromosome'].append(details['Chromosome'])\n",
        "        new_data_frame['WT_sequence'].append(details['WT_sequence'])  # Adding WT_sequence to each row\n",
        "\n",
        "    return pd.DataFrame(new_data_frame)\n",
        "\n",
        "# Define a function to calculate exon_start and exon_end\n",
        "def calculate_exon_positions(row):\n",
        "    if row['strand_orientation'] == -1:\n",
        "        exon_start = row['padded_chrom_start'] + (expand_3prime+1)\n",
        "        exon_end = row['padded_chrom_end'] - (expand_5prime+1)\n",
        "    else:\n",
        "        exon_start = row['padded_chrom_start'] + (expand_5prime+1)\n",
        "        exon_end = row['padded_chrom_end'] - (expand_3prime+1)\n",
        "    return pd.Series([exon_start, exon_end], index=['exon_start', 'exon_end'])\n",
        "\n",
        "    server = \"https://rest.ensembl.org\"\n",
        "def get_canonical_transcript_id(gene_name):\n",
        "    # Base URL for the Ensembl REST API\n",
        "    server = \"https://rest.ensembl.org\"\n",
        "\n",
        "    # Endpoint for lookup by gene symbol\n",
        "    ext_lookup = f\"/lookup/symbol/homo_sapiens/{ensembl_gene_name_query}?expand=1\"\n",
        "\n",
        "    # Make the request to get gene data\n",
        "    response = requests.get(server + ext_lookup, headers={\"Content-Type\": \"application/json\"})\n",
        "\n",
        "    if not response.ok:\n",
        "        response.raise_for_status()\n",
        "        sys.exit()\n",
        "\n",
        "    decoded = response.json()\n",
        "\n",
        "    # Extract the canonical transcript ID\n",
        "    for transcript in decoded.get('Transcript', []):\n",
        "        if transcript.get('is_canonical', 0) == 1:\n",
        "            return transcript['id']\n",
        "    return None\n",
        "\n",
        "def get_exon_id_list(transcript_id: str) -> list[str]:\n",
        "    tr_details = _fetch_feature_details(transcript_id)\n",
        "    exon_list = tr_details.get('Exon')\n",
        "    return [e.get('id') for e in exon_list]\n",
        "\n",
        "def _fetch_feature_details(stable_id: str) -> dict:\n",
        "    ext = f\"/lookup/id/{stable_id}?expand=1\"\n",
        "    return _get_from_REST(ext)\n",
        "\n",
        "def _get_from_REST(endpoint: str) -> dict:\n",
        "    r = requests.get(server+endpoint, headers={\"Content-Type\": \"application/json\"})\n",
        "    if not r.ok:\n",
        "        r.raise_for_status()\n",
        "    return r.json()\n",
        "\n",
        "def get_specific_exon_id(transcript_id: str, exon_number_query: int) -> str:\n",
        "    exon_ids = get_exon_id_list(transcript_id)\n",
        "\n",
        "    # Adjust for zero-based indexing and check if the index is in range\n",
        "    if 0 < exon_number_query <= len(exon_ids):\n",
        "        return exon_ids[exon_number_query - 1]\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iZlmRHgS-yCG",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Query Ensembl\n",
        "\n",
        "server = \"https://rest.ensembl.org\"\n",
        "if ensembl_exon_id == \"\":\n",
        "     canonical_transcript_id = get_canonical_transcript_id(ensembl_gene_name_query)\n",
        "     ensembl_exon_id = get_specific_exon_id(canonical_transcript_id, exon_number_query)\n",
        "else:\n",
        "  ensembl_exon_id == ensembl_exon_id\n",
        "def get_exon_info(exon_id, expand_3prime, expand_5prime):\n",
        "    ext = f\"/sequence/id/{exon_id}?type=genomic;expand_3prime={expand_3prime+1};expand_5prime={expand_5prime+1}\"\n",
        "    response = requests.get(server + ext, headers={\"Content-Type\": \"text/x-fasta\"})\n",
        "\n",
        "    if not response.ok:\n",
        "        response.raise_for_status()\n",
        "\n",
        "    fasta_data = response.text\n",
        "    header_line, sequence = fasta_data.split('\\n', 1)[0], ''.join(fasta_data.split('\\n')[1:])\n",
        "\n",
        "    # Extract metadata from the header line\n",
        "    parts = header_line.split(':')\n",
        "    chromosome, padded_chrom_start, padded_chrom_end, strand_orientation = parts[2], int(parts[3]), int(parts[4]), int(parts[5])\n",
        "\n",
        "\n",
        "    return {\n",
        "        'exon_id': exon_id,\n",
        "        'chromosome': chromosome,\n",
        "        'padded_chrom_start': padded_chrom_start,\n",
        "        'padded_chrom_end': padded_chrom_end,\n",
        "        'strand_orientation': strand_orientation,\n",
        "        'padded_exon_sequence': sequence\n",
        "    }\n",
        "exon_info = get_exon_info(ensembl_exon_id, expand_3prime, expand_5prime)\n",
        "exon_info_df = pd.DataFrame([exon_info])\n",
        "\n",
        "# Apply the function to calculate exon positions\n",
        "exon_info_df[['exon_start', 'exon_end']] = exon_info_df.apply(calculate_exon_positions, axis=1)\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming exon_info_df is already prepared with 'exon_start' and 'exon_end' columns\n",
        "# Initialize an empty DataFrame to store position lookup information for all exons\n",
        "all_position_lookups = pd.DataFrame()\n",
        "\n",
        "for index, row in exon_info_df.iterrows():\n",
        "    exon_id = row['exon_id']\n",
        "    padded_exon_sequence = row['padded_exon_sequence']\n",
        "    chromosome = row['chromosome']\n",
        "    padded_chrom_start = row['padded_chrom_start']\n",
        "    padded_chrom_end = row['padded_chrom_end']\n",
        "    exon_start = row['exon_start']  # Assuming this column exists\n",
        "    exon_end = row['exon_end']  # Assuming this column exists\n",
        "\n",
        "    positions = list(range(1, len(padded_exon_sequence) + 1))\n",
        "    nucleotides = list(padded_exon_sequence)\n",
        "    chromosomes = [chromosome] * len(positions)\n",
        "\n",
        "    # Calculate reference positions\n",
        "    reference_positions = list(range(padded_chrom_start, padded_chrom_start + len(padded_exon_sequence))) if row['strand_orientation'] != -1 else list(range(padded_chrom_end, padded_chrom_end - len(padded_exon_sequence), -1))[::-1]\n",
        "\n",
        "    # Determine regions (EXON or INTRON)\n",
        "    regions = ['EXON' if exon_start <= pos <= exon_end else 'INTRON' for pos in reference_positions]\n",
        "\n",
        "    # Create DataFrame for current exon\n",
        "    position_lookup = pd.DataFrame({\n",
        "        'position': positions,  # Starting from 1\n",
        "        'nucleotide': nucleotides,\n",
        "        'chromosome': chromosomes,\n",
        "        'reference position': reference_positions,\n",
        "        'exon ID': [exon_id] * len(positions),\n",
        "        'region': regions  # Add region based on the condition\n",
        "    })\n",
        "\n",
        "    # Append to the all_position_lookups DataFrame\n",
        "    all_position_lookups = pd.concat([all_position_lookups, position_lookup], ignore_index=True)\n",
        "\n",
        "# Now all_position_lookups contains position lookup information for all exons, with indexing starting from 1\n",
        "# Optionally, save this DataFrame to a CSV file\n",
        "all_position_lookups.index = all_position_lookups.index + 1  # Add 1 to the index\n",
        "#all_position_lookups\n",
        "# Define the headers and the row of data\n",
        "headers = [\"Ensembl Gene Name\", \"Exon Number\",\"Ensembl Exon ID\", \"Intronic 3 prime inclusion\", \"Intronic 5 prime inclusion\", \"Deletions\", \"Substitutions\"]\n",
        "data_row = [\n",
        "    ensembl_gene_name_query,\n",
        "    exon_number_query,\n",
        "    f\"{expand_3prime} nt\",\n",
        "    f\"{expand_5prime} nt\",\n",
        "    deletion_sizes,\n",
        "    \"Yes\" if include_all_possible_substitutions else \"No\"\n",
        "]\n",
        "\n",
        "# Write to a CSV file\n",
        "query_csv_file_path = f\"{ensembl_gene_name_query}_{exon_number_query}_{ensembl_exon_id}_query.csv\"\n",
        "with open(query_csv_file_path, mode='w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(headers)\n",
        "    writer.writerow(data_row)\n",
        "\n",
        "exon_info_df"
      ],
      "metadata": {
        "id": "f5Ozdgvu_UDz",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Create indel library and encode as VCF file for input to splice AI\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "import pandas as pd\n",
        "from Bio.Seq import Seq\n",
        "import re\n",
        "\n",
        "# Assuming exon_info_df and deletion_sizes are already defined\n",
        "\n",
        "# Combined DataFrame initialization\n",
        "combined_variants_df = pd.DataFrame()\n",
        "\n",
        "# Iterate over each exon\n",
        "for index, row in exon_info_df.iterrows():\n",
        "    exon_id = row['exon_id']\n",
        "    sequence = row['padded_exon_sequence']\n",
        "    strand_orientation = row['strand_orientation']\n",
        "    chromosome = row['chromosome']\n",
        "    padded_chrom_start = row['padded_chrom_start']  # Extract padded_chrom_start from the row\n",
        "\n",
        "    # Determine the WT sequence based on strand orientation\n",
        "    WT = str(Seq(sequence).reverse_complement()) if strand_orientation == -1 else sequence\n",
        "\n",
        "if include_all_possible_substitutions:\n",
        "    # Generate all single nucleotide substitutions\n",
        "    singles_df = generate_all_singles(WT, exon_id, chromosome, padded_chrom_start)\n",
        "    singles_df['Exon_id'] = exon_id  # Add the exon_id to each substitution\n",
        "    singles_df['Chromosome'] = chromosome  # Add the chromosome number to each substitution\n",
        "    singles_df['WT_sequence'] = WT  # Add the WT sequence for each substitution\n",
        "else:\n",
        "    # Create an empty DataFrame with the same columns as deletions_df if needed\n",
        "    singles_df = pd.DataFrame(columns=['ID', 'Sequence', 'Exon_id', 'Chromosome', 'WT_sequence'])\n",
        "\n",
        "    # Generate all deletions\n",
        "if isinstance(deletion_sizes, str):\n",
        "    deletion_sizes = [int(size) for size in deletion_sizes.split(',')]\n",
        "\n",
        "deletions_list = []\n",
        "\n",
        "\n",
        "for each_del_size in deletion_sizes:\n",
        "    # Ensure each_del_size is an integer\n",
        "    each_del_size_int = int(each_del_size)  # Convert to integer if not already\n",
        "    variants_with_kmer_deletion = generate_all_deletions(each_del_size_int, WT, exon_id, chromosome, padded_chrom_start)\n",
        "    for variant_id, variant_seq in zip(variants_with_kmer_deletion['ID'], variants_with_kmer_deletion['Sequence']):\n",
        "        deletions_list.append({'ID': variant_id, 'Sequence': variant_seq, 'Exon_id': exon_id, 'Chromosome': chromosome, 'WT_sequence': WT})\n",
        "\n",
        "    # Generate all deletions})\n",
        "    deletions_df = pd.DataFrame(deletions_list)\n",
        "\n",
        "    # Combine substitutions and deletions for this exon\n",
        "    exon_variants_df = pd.concat([singles_df, deletions_df], ignore_index=True)\n",
        "\n",
        "    # Concatenate the results for all exons\n",
        "    combined_variants_df = pd.concat([combined_variants_df, exon_variants_df], ignore_index=True)\n",
        "\n",
        "# Remove duplicated sequences and combine IDs\n",
        "combined_variants_df = remove_duplicates_in_variant_data_frame(combined_variants_df)\n",
        "\n",
        "# Save the combined DataFrame to a CSV file\n",
        "combined_variants_df.to_csv(\"combined_variants_data.csv\", index=False)\n",
        "def extract_individual_variants_info(variants_df, variant_type='_Del_'):\n",
        "    # Initialize lists to store extracted information\n",
        "    CHROM, POS, ID, REF, ALT, WT_SEQ = [], [], [], [], [], []\n",
        "\n",
        "    # Filter rows based on variant type\n",
        "    filtered_variants_df = variants_df[variants_df['ID'].str.contains(variant_type, regex=False)]\n",
        "\n",
        "    # Iterate over each row in the filtered DataFrame\n",
        "    for index, row in filtered_variants_df.iterrows():\n",
        "        # Split ID if it contains multiple variants joined together\n",
        "        ids = row['ID'].split(';')\n",
        "        for individual_id in ids:\n",
        "            WT_sequence = row['WT_sequence']\n",
        "            # Ensure chrom_info extraction is robust\n",
        "            chrom_match = re.search(r\"chr(\\d+|\\w+)\", individual_id)  # Adjusted to match more than just numbers, e.g., X, Y chromosomes\n",
        "            if chrom_match:\n",
        "                chrom_info = chrom_match.group(1)\n",
        "                padded_chrom_start = int(individual_id.split(':')[1].split('_')[0])\n",
        "\n",
        "                if variant_type == '_Del_':\n",
        "                    # Process deletions\n",
        "                    del_info = re.search(r\"_Del_k(\\d+)_(\\d+)-(\\d+)\", individual_id)\n",
        "                    if del_info:\n",
        "                        deletion_start_pos, deletion_end_pos = int(del_info.group(2)), int(del_info.group(3))\n",
        "                        start_pos_abs = padded_chrom_start + deletion_start_pos - 2\n",
        "                        REF_seq = WT_sequence[deletion_start_pos - 2:deletion_end_pos] if deletion_start_pos - 2 < len(WT_sequence) else \".\"\n",
        "                        ALT_value = REF_seq[0] if REF_seq else \".\"\n",
        "                else:\n",
        "                    # Process substitutions\n",
        "                    sub_info = re.search(r\"_Sub_([ATGC])(\\d+)([ATGC])\", individual_id)\n",
        "                    if sub_info:\n",
        "                        ref_nucleotide, position_of_substitution, alt_nucleotide = sub_info.group(1), int(sub_info.group(2)), sub_info.group(3)\n",
        "                        start_pos_abs = padded_chrom_start + position_of_substitution - 1\n",
        "                        REF_seq, ALT_value = ref_nucleotide, alt_nucleotide\n",
        "\n",
        "                # Append to lists\n",
        "                CHROM.append(chrom_info)  # Keep as string to accommodate 'X', 'Y', etc.\n",
        "                POS.append(start_pos_abs)\n",
        "                ID.append(individual_id)\n",
        "                REF.append(REF_seq)\n",
        "                ALT.append(ALT_value)\n",
        "                WT_SEQ.append(WT_sequence)\n",
        "\n",
        "    # Create the new DataFrame based on variant type\n",
        "    info_df = pd.DataFrame({\n",
        "        'CHROM': CHROM,\n",
        "        'POS': [int(pos) for pos in POS],\n",
        "        'ID': ID,\n",
        "        'REF': REF,\n",
        "        'ALT': ALT,\n",
        "        'WT_sequence': WT_SEQ\n",
        "    })\n",
        "\n",
        "    return info_df\n",
        "\n",
        "  # Usage\n",
        "deletion_info_df = extract_individual_variants_info(combined_variants_df, variant_type='_Del_')\n",
        "substitution_info_df = extract_individual_variants_info(combined_variants_df, variant_type='_Sub_')\n",
        "\n",
        "\n",
        "# Combine deletion and substitution information into one DataFrame\n",
        "combined_vcf_info_df = pd.concat([deletion_info_df, substitution_info_df], ignore_index=True)\n",
        "# After constructing or modifying the DataFrame\n",
        "combined_vcf_info_df['POS'] = combined_vcf_info_df['POS'].astype(int)\n",
        "\n",
        "# Filter out specific rows if necessary (e.g., based on REF values)\n",
        "# This step is optional and depends on your specific requirements\n",
        "# Filter out rows with empty or whitespace-only values in the 'REF' column\n",
        "filtered_vcf_info_df = combined_vcf_info_df[combined_vcf_info_df['REF'].str.strip() != \"\"]\n",
        "filtered_vcf_info_df = filtered_vcf_info_df.drop_duplicates()\n",
        "\n",
        "# Define the VCF header\n",
        "vcf_header = \"\"\"##fileformat=VCFv4.2\n",
        "##reference=GRCh38/hg38\n",
        "##contig=<ID=chr1,length=249250621>\n",
        "##contig=<ID=chr2,length=243199373>\n",
        "##contig=<ID=chr3,length=198022430>\n",
        "##contig=<ID=chr4,length=191154276>\n",
        "##contig=<ID=chr5,length=180915260>\n",
        "##contig=<ID=chr6,length=171115067>\n",
        "##contig=<ID=chr7,length=159138663>\n",
        "##contig=<ID=chr8,length=146364022>\n",
        "##contig=<ID=chr9,length=141213431>\n",
        "##contig=<ID=chr10,length=135534747>\n",
        "##contig=<ID=chr11,length=135006516>\n",
        "##contig=<ID=chr12,length=133851895>\n",
        "##contig=<ID=chr13,length=115169878>\n",
        "##contig=<ID=chr14,length=107349540>\n",
        "##contig=<ID=chr15,length=102531392>\n",
        "##contig=<ID=chr16,length=90354753>\n",
        "##contig=<ID=chr17,length=81195210>\n",
        "##contig=<ID=chr18,length=78077248>\n",
        "##contig=<ID=chr19,length=59128983>\n",
        "##contig=<ID=chr20,length=63025520>\n",
        "##contig=<ID=chr21,length=48129895>\n",
        "##contig=<ID=chr22,length=51304566>\n",
        "##contig=<ID=chrX,length=155270560>\n",
        "##contig=<ID=chrY,length=59373566>\n",
        "#CHROM\\tPOS\\tID\\tREF\\tALT\\tQUAL\\tFILTER\\tINFO\"\"\"\n",
        "\n",
        "# Format each row into a VCF entry\n",
        "vcf_rows = []\n",
        "for _, row in filtered_vcf_info_df.iterrows():\n",
        "    vcf_entry = f\"chr{row['CHROM']}\\t{row['POS']}\\t{row['ID']}\\t{row['REF']}\\t{row['ALT']}\\t30\\tPASS\\t.\"\n",
        "    vcf_rows.append(vcf_entry)\n",
        "# Filter VCF rows to exclude REF = \"t\" where there is missing reference base\n",
        "vcf_rows = [row for row in vcf_rows if not row.endswith(\"\\t\\t.\\t30\\tPASS\\t.\")]\n",
        "# Define the filename for the VCF file\n",
        "vcf_filename = f\"{ensembl_gene_name_query}_{exon_number_query}_{ensembl_exon_id}_combined_variants.vcf\"\n",
        "\n",
        "# Write the VCF file\n",
        "with open(vcf_filename, 'w') as vcf_file:\n",
        "    vcf_file.write(vcf_header + '\\n')\n",
        "    vcf_file.writelines('\\n'.join(vcf_rows))\n",
        "\n",
        "print(f\"VCF file '{vcf_filename}' has been created.\")\n"
      ],
      "metadata": {
        "id": "KcLT5NBXuNva",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Run SpliceAI and save output on machine\n",
        "output_filename = f\"{ensembl_gene_name_query}_{exon_number_query}_{ensembl_exon_id}_3prime{expand_3prime}_5prime{expand_5prime}_ntw{window}_spliceai_output.vcf\"\n",
        "!spliceai -I {vcf_filename} -O {output_filename} -R hg{hg}.fa -A grch{GRCh} -D {window}\n",
        "if automatic_download == True :\n",
        "     from google.colab import files\n",
        "     files.download(output_filename)\n",
        "     files.download(query_csv_file_path)\n",
        "spliceAI_table = pd.read_csv(output_filename, skiprows=28, sep='\\t') ##chnage to name of output\n",
        "spliceAI_table = spliceAI_table.drop_duplicates()"
      ],
      "metadata": {
        "id": "rsS5gkOaLoQb",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Generate triangular heatmap of all in silico deletions\n",
        "import pandas as pd\n",
        "def extract_max_delta_score(info):\n",
        "    # Extract the SpliceAI scores part from the INFO column\n",
        "    spliceai_scores = info.split(';')[0]  # Assuming SpliceAI scores are the first in the INFO field\n",
        "    delta_scores = spliceai_scores.split('|')[2:6]\n",
        "    delta_scores = [float(score) for score in delta_scores]\n",
        "\n",
        "    # Check if all scores are zero\n",
        "    if all(score == 0 for score in delta_scores):\n",
        "        return 0\n",
        "\n",
        "    # Find the maximum delta score\n",
        "    max_score = max(delta_scores)\n",
        "\n",
        "    # Find the index of the first occurrence of the maximum score\n",
        "    max_score_index = delta_scores.index(max_score)\n",
        "\n",
        "    # Adjust the result based on the index\n",
        "    result = delta_scores[max_score_index]\n",
        "    if max_score_index in [1, 3]:\n",
        "        result *= -1\n",
        "\n",
        "    return result\n",
        "\n",
        "def extract_exon_id(id_string):\n",
        "    # Split the string by underscores\n",
        "    parts = id_string.split('_')\n",
        "    # Check if there are enough parts to contain an exon ID\n",
        "    if len(parts) > 1:\n",
        "        # Remove the \"ex\" prefix and return the exon ID\n",
        "        exon_id = parts[1][2:] if parts[1].startswith(\"ex\") else parts[1]\n",
        "        return exon_id\n",
        "    else:\n",
        "        # Return None if no exon ID is found\n",
        "        return None\n",
        "\n",
        "spliceAI_table['predictions'] = spliceAI_table['INFO'].apply(extract_max_delta_score)\n",
        "# Assuming spliceAI_table is a pandas DataFrame with an 'ID' column\n",
        "spliceAI_table['exon_id'] = spliceAI_table['ID'].apply(extract_exon_id)\n",
        "\n",
        "\n",
        "vector_starts = []\n",
        "vector_ends = []\n",
        "vector_delta_psi = []\n",
        "\n",
        "# Extract information from IDs in the spliceAI_table\n",
        "for index, row in spliceAI_table.iterrows():\n",
        "    id_info = row['ID']\n",
        "    # Check if ID format matches deletion pattern\n",
        "    match = re.search(r\"Del_k(\\d+)_(\\d+)-(\\d+)\", id_info)\n",
        "    if match:\n",
        "        # Extract deletion size and start position from the ID\n",
        "        deletion_size = int(match.group(1))\n",
        "        start_position = int(match.group(2))\n",
        "        end_position = int(match.group(3))\n",
        "        # Append extracted information to lists\n",
        "        vector_starts.append(start_position)\n",
        "        vector_ends.append(end_position)\n",
        "        vector_delta_psi.append(row['predictions'])\n",
        "\n",
        "# Prepare deletion heatmap data\n",
        "heatmap_df = pd.DataFrame({\n",
        "    'start': vector_starts,\n",
        "    'end': vector_ends,\n",
        "    'dPSI': vector_delta_psi\n",
        "})\n",
        "heatmap_df\n",
        "\n",
        "# Create custom colormap and set color limits\n",
        "my_colours = sns.color_palette(\"Spectral_r\", 11)\n",
        "cmap = LinearSegmentedColormap.from_list(\"my_colormap\", my_colours)\n",
        "\n",
        "# Create heatmap with flipped data\n",
        "plt.figure(figsize=(12, 12))\n",
        "\n",
        "# Flip the data by transposing the DataFrame\n",
        "flipped_df = heatmap_df.pivot(\"start\", \"end\", \"dPSI\")\n",
        "\n",
        "# Set x-axis ticks at every 5th number\n",
        "x_ticks = np.arange(0, len(flipped_df.columns), 5)\n",
        "#ax = sns.heatmap(flipped_df, cmap=cmap, cbar_kws={'label': 'Max (Splice AI delta score)', 'location': 'left'}, vmin=-0.5, vmax=0.25, xticklabels=x_ticks, yticklabels=True)\n",
        "\n",
        "\n",
        "# Assuming flipped_df is your DataFrame and it contains a column named 'vector_delta_psi'\n",
        "min_delta_psi = heatmap_df['dPSI'].min()\n",
        "max_delta_psi = heatmap_df['dPSI'].max()\n",
        "\n",
        "# Create the heatmap\n",
        "ax = sns.heatmap(flipped_df, cmap=cmap, cbar_kws={'label': 'Max (Splice AI delta score)', 'location': 'left'}, vmin=min_delta_psi, vmax=max_delta_psi, xticklabels=x_ticks, yticklabels=True)\n",
        "# Assuming position_lookup already includes an 'exon_id' column\n",
        "#position_to_label = all_position_lookups.apply(lambda x: ((x['exon ID'],x['position']), f\"{x['chromosome']}:{x['reference position']}:{x['nucleotide']}\"), axis=1).to_dict()\n",
        "# Create the position_to_label dictionary starting at 1 for keys\n",
        "#position_to_label = all_position_lookups.apply(lambda x: ((x['exon ID'], x['position']), f\"{x['chromosome']}:{x['reference position']}:{x['nucleotide']}\"), axis=1).to_dict()\n",
        "position_to_label = all_position_lookups.apply(\n",
        "    lambda x: ((x['exon ID'], x['position']), f\"{x['chromosome']}:{x['reference position']}:{x['nucleotide']} ({x['region']})\"),\n",
        "    axis=1\n",
        ").to_dict()\n",
        "\n",
        "\n",
        "#position_to_label = all_position_lookups.apply(lambda x: ((x['exon ID'], x['position']), f\"{x['chromosome']}:{x['reference position']}:{x['nucleotide']} ({x['region']})\"), axis=1).to_dict()\n",
        "# Assuming all_position_lookups is a DataFrame with columns 'exon ID', 'position', 'chromosome', 'reference position', 'nucleotide'\n",
        "\n",
        "# Update x-axis ticks with labels from the position_lookup mapping\n",
        "# Example usage of set_xticklabels with a dictionary that has tuple keys\n",
        "ax.set_xticklabels([position_to_label.get(( pos), '') for pos in flipped_df.columns[x_ticks]], rotation=90)\n",
        "\n",
        "\n",
        "# Set y-axis ticks at every 5th number\n",
        "y_ticks = np.arange(0, len(flipped_df.index), 5)\n",
        "ax.set_yticks(y_ticks)\n",
        "ax.set_yticklabels([position_to_label.get(pos, '') for pos in flipped_df.index[y_ticks]], rotation=0)\n",
        "\n",
        "\n",
        "# Assuming position_lookup is your DataFrame with columns 'position', 'chromosome', 'reference_position', 'nucleotide'\n",
        "# Create a mapping from position to formatted string\n",
        "\n",
        "\n",
        "# Create a secondary x-axis at the top\n",
        "ax2 = ax.secondary_xaxis('top')\n",
        "ax2.set_xticks(x_ticks)\n",
        "# Example usage of set_xticklabels with a dictionary that has tuple keys\n",
        "#ax2.set_xticklabels([(lambda x: position_to_label.get(x, '')[1])(x) for x in x_ticks], rotation=90)\n",
        "ax2.set_xticklabels([(lambda x: position_to_label.get(x, '')[1] if position_to_label.get(x, '') else '')(x) for x in x_ticks], rotation=90)\n",
        "\n",
        "ax3 = ax.secondary_yaxis('right')\n",
        "ax3.set_yticks(y_ticks)\n",
        "\n",
        "ax3.set_yticklabels([(lambda y: position_to_label.get(y, '')[1] if position_to_label.get(y, '') else '')(y) for y in y_ticks])\n",
        "\n",
        "#ax3.set_yticklabels([(lambda y: position_to_label.get(y, '')[1])(y) for y in y_ticks])\n",
        "\n",
        "# Remove the bottom x-axis\n",
        "ax.set_xlabel('')\n",
        "ax.set_xticklabels([])\n",
        "\n",
        "# Remove the left-hand side y-axis\n",
        "ax.set_ylabel('')\n",
        "ax.set_yticklabels([])\n",
        "\n",
        "# Set axis labels for the right y-axis and top x-axis\n",
        "ax3.set_ylabel(\"First Deleted Position\")\n",
        "ax2.set_xlabel(\"Last Deleted Position\")\n",
        "plt.title(f\"In silico DIM with spliceAI, all deletions, - Gene: {ensembl_gene_name_query}, Exon Number: {exon_number_query}, nt window :{window}_heatmap.pdf\")\n",
        "\n",
        "\n",
        "# Save heatmap\n",
        "\n",
        "filename_heatmap = f\"{ensembl_gene_name_query}_Exon_{exon_number_query}_{ensembl_exon_id}_ntw_{window}_heatmap.pdf\"\n",
        "plt.savefig(filename_heatmap, bbox_inches='tight')\n",
        "if automatic_download == True :\n",
        "     from google.colab import files\n",
        "     files.download(filename_heatmap)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "X-nlxuimKSFv",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Generate plot spliceAI predictions for 1-6nt deletions with fitted LOESS curve to exonic region\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import statsmodels.api as sm\n",
        "\n",
        "\n",
        "# Initialize lists\n",
        "spliceAI_vector = []\n",
        "position_vector = []\n",
        "k_vector = []\n",
        "\n",
        "# Determine the length of WT\n",
        "wt_length = len(WT)  # Replace with actual WT length\n",
        "\n",
        "# Sliding window size 1-10\n",
        "for k in range(1, 11):\n",
        "    for p in range(1, wt_length - k + 1):\n",
        "        this_id = f\"Del_k{k}_{p}-{p + k - 1}\"\n",
        "        idx = spliceAI_table[spliceAI_table['ID'].str.contains(this_id)].index\n",
        "\n",
        "        if len(idx) > 0:\n",
        "            spliceAI_vector.extend(spliceAI_table.loc[idx, 'predictions'])\n",
        "            position_vector.extend([p + (k / 2) - 0.5] * len(idx))\n",
        "            k_vector.extend([k] * len(idx))\n",
        "\n",
        "# Create DataFrame\n",
        "plot_df = pd.DataFrame({\n",
        "    'spliceAI': spliceAI_vector,\n",
        "    'k': k_vector,\n",
        "    'position': position_vector\n",
        "})\n",
        "\n",
        "# Filter for k in range 1-6\n",
        "plot_df_filtered = plot_df[plot_df['k'].isin(range(1, 7))]\n",
        "\n",
        "# Define color palette\n",
        "my_shapes = ['o', 's', 'D', '^', 'v', 'P']\n",
        "\n",
        "# Filter the DataFrame for fitting the LOESS curve TO BE FINALISED\n",
        "plot_df_loess = plot_df_filtered[(plot_df_filtered['position'] >= expand_5prime) & (plot_df_filtered['position'] <= wt_length - expand_3prime)]\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Add lighter grey shading for potential silencer regions\n",
        "lowess = sm.nonparametric.lowess(plot_df_loess['spliceAI'], plot_df_loess['position'], frac=0.15)\n",
        "lowess_x, lowess_y = zip(*lowess)\n",
        "for x, y in zip(lowess_x, lowess_y):\n",
        "    if y > 0:\n",
        "        plt.axvspan(x - 0.5, x + 0.5, color='lightgray', alpha=0.3)\n",
        "\n",
        "# Plot individual points with different shapes for all data\n",
        "for i, k in enumerate(range(1, 7)):\n",
        "    data = plot_df_filtered[plot_df_filtered['k'] == k]\n",
        "    sns.scatterplot(data=data, x='position', y='spliceAI', marker=my_shapes[i], label=f'Deletion Length {k} nt')\n",
        "\n",
        "# Plot LOESS curve in a darker golden color\n",
        "plt.plot(lowess_x, lowess_y, color='#FFA500', linewidth=2, label='LOESS Curve')\n",
        "\n",
        "# Compute and plot the 95% confidence interval for LOESS\n",
        "confidence = 1.96 * np.std(plot_df_loess['spliceAI']) / np.sqrt(len(plot_df_loess['spliceAI']))\n",
        "plt.fill_between(lowess_x, lowess_y - confidence, lowess_y + confidence, color='#FFA500', alpha=0.5)\n",
        "\n",
        "# Add a horizontal line at y=0\n",
        "plt.axhline(0, color='gray', linestyle='--', linewidth=1)\n",
        "\n",
        "# Use the position-to-label mapping for x-axis ticks\n",
        "x_ticks = np.arange(min(plot_df['position']), max(plot_df['position']) + 1, step=2)\n",
        "x_tick_labels = ([(lambda x: position_to_label.get(x, '')[1])(x) for x in x_ticks])\n",
        "plt.xticks(x_ticks, x_tick_labels, rotation=90)\n",
        "\n",
        "# Add a dummy plot entry for \"Silencer\" and \"Enhancer\" in legend\n",
        "plt.scatter([], [], color='lightgray', alpha=0.3, label='Silencer')\n",
        "plt.scatter([], [], color='white', label='Enhancer')\n",
        "\n",
        "plt.legend(loc='upper left')\n",
        "\n",
        "# Adding Ensembl Gene Name and Exon Number to the title\n",
        "plt.title(f\"In silico DIM with spliceAI, fitted Loess to 1-6nt deletions, - Gene: {ensembl_gene_name_query}, Exon Number: {exon_number_query}, Ensembl exon ID : {ensembl_exon_id} nt window : {window}\")\n",
        "\n",
        "# Saving and showing the plot\n",
        "filename_plot = f\"{ensembl_gene_name_query}_Exon_{exon_number_query}_{ensembl_exon_id}_ntw_{window}_loess_plot.pdf\"\n",
        "plt.savefig(filename_plot, bbox_inches='tight')\n",
        "if automatic_download == True :\n",
        "     from google.colab import files\n",
        "     files.download(filename_plot)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IOVK8neLrRBI",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Plot longitudinal heatmap of deletions and substitutions\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "\n",
        "##need to define sub_df\n",
        "# Assuming spliceAI_table is already loaded and has 'predictions' column calculated\n",
        "# Initialize lists\n",
        "vector_starts = []\n",
        "deletion_sizes = []\n",
        "vector_delta_psi = []\n",
        "\n",
        "# Extract information from IDs in the spliceAI_table\n",
        "for index, row in spliceAI_table.iterrows():\n",
        "    id_info = row['ID']\n",
        "    # Check if ID format matches deletion pattern\n",
        "    match = re.search(r\"Del_k(\\d+)_(\\d+)-(\\d+)\", id_info)\n",
        "    if match:\n",
        "        # Extract deletion size and start position from the ID\n",
        "        deletion_size = int(match.group(1))\n",
        "        start_position = int(match.group(2))\n",
        "\n",
        "        # Append extracted information to lists\n",
        "        vector_starts.append(start_position)\n",
        "        deletion_sizes.append(deletion_size)\n",
        "        vector_delta_psi.append(row['predictions'])\n",
        "\n",
        "# Prepare deletion heatmap data\n",
        "del_heatmap_df = pd.DataFrame({\n",
        "    'start': vector_starts,\n",
        "    'deletion_size': deletion_sizes,\n",
        "    'dPSI': vector_delta_psi\n",
        "})\n",
        "# Transpose the pivot for deletions to have deletion size on x-axis and start position on y-axis\n",
        "del_pivot_df = del_heatmap_df.pivot(\"deletion_size\", \"start\", \"dPSI\")\n",
        "\n",
        "# Initialize lists to store parsed data\n",
        "positions = []\n",
        "refs = []\n",
        "alts = []\n",
        "predictions = []\n",
        "\n",
        "# Regex pattern to extract position, REF, and ALT from ID\n",
        "pattern = re.compile(r\"Sub_([ATCG])(\\d+)([ATCG])\")\n",
        "\n",
        "for idx, row in spliceAI_table.iterrows():\n",
        "    match = pattern.search(row['ID'])\n",
        "    if match:\n",
        "        ref, pos, alt = match.groups()\n",
        "        positions.append(int(pos))\n",
        "        refs.append(ref)\n",
        "        alts.append(alt)\n",
        "        predictions.append(row['predictions'])  # Assuming 'predictions' column exists\n",
        "\n",
        "# Create DataFrame\n",
        "sub_df = pd.DataFrame({\n",
        "    'Position': positions,\n",
        "    'REF': refs,\n",
        "    'ALT': alts,\n",
        "    'Prediction': predictions\n",
        "})\n",
        "\n",
        "# Prepare substitution heatmap data\n",
        "sub_df_filtered = sub_df[sub_df['Position'] != 1]\n",
        "sub_pivot_df = sub_df_filtered.pivot_table(index='ALT', columns='Position', values='Prediction', aggfunc='mean')\n",
        "\n",
        "# Color scale limits\n",
        "vmin, vmax = -0.5, 0.5  # Specified color scale limits\n",
        "\n",
        "# Combine both heatmaps vertically with a shared color scale\n",
        "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(20, 12), sharex=True, gridspec_kw={'height_ratios': [1, 3]})\n",
        "\n",
        "# Plot deletion heatmap\n",
        "sns.heatmap(del_pivot_df, cmap=\"Spectral_r\", cbar=True, ax=ax1, vmin=vmin, vmax=vmax, cbar_kws={'label': 'SpliceAI Prediction'})\n",
        "ax1.set_title(\"Deletions\")\n",
        "ax1.set_xlabel(\"\")\n",
        "ax1.set_ylabel(\"Deletion Size (nt)\")\n",
        "\n",
        "# Plot substitution heatmap\n",
        "sns.heatmap(sub_pivot_df, cmap=\"Spectral_r\", cbar=True, ax=ax2, vmin=vmin, vmax=vmax, cbar_kws={'label': 'SpliceAI Prediction'}, annot=False)\n",
        "ax2.set_title(\"Substitutions\")\n",
        "ax2.set_xlabel(\"Position\")\n",
        "ax2.set_ylabel(\"ALT Base\")\n",
        "\n",
        "# Adjust x-axis ticks to shift to the right by 0.5\n",
        "x_ticks = np.arange(0.5, len(sub_pivot_df.columns) + 0.5, 1)  # Shifting ticks to the right by 0.5\n",
        "x_tick_labels = [position_to_label.get(x+1, '') for x in range(len(sub_pivot_df.columns))]  # Adjusting for shifted ticks\n",
        "\n",
        "ax2.set_xticks(x_ticks)\n",
        "ax2.set_xticklabels(x_tick_labels, rotation=90)\n",
        "# Annotate NaN values with asterisks in substitution heatmap\n",
        "for ytick, row in enumerate(sub_pivot_df.itertuples(index=False)):\n",
        "    for xtick, value in enumerate(row):\n",
        "        if pd.isna(value):  # Check for NaN\n",
        "            ax2.text(xtick + 0.5, ytick + 0.5, '*', ha='center', va='center', color='black')  # Adjusted position for annotation\n",
        "\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "filename_heatmap_long = f\"{ensembl_gene_name_query}_Exon_{exon_number_query}_{ensembl_exon_id}_ntw_{window}_heatmap_long.pdf\"\n",
        "plt.title(f\"In silico DIM with spliceAI, all deletions & substitutions, - Gene: {ensembl_gene_name_query}, Exon Number: {exon_number_query}, Ensembl exon ID : {ensembl_exon_id} nt window :{window}_heatmap_long\")\n",
        "plt.savefig(filename_heatmap_long, bbox_inches='tight')\n",
        "if automatic_download == True :\n",
        "     from google.colab import files\n",
        "     files.download(filename_heatmap_long)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "tIZ9QYU4BIjT",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}